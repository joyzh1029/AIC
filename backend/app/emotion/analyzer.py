import cv2
import threading
import time
import numpy as np
from collections import deque
from queue import Queue
from PIL import Image

from app.multimodal.vlm import summarize_scene
from app.audio.stt import record_audio, transcribe_audio
from backend.app.emotion.fer_emotion import analyze_facial_expression
from backend.app.emotion.ser_emotion import analyze_voice_emotion_korean
from backend.app.nlp.llm import generate_response
from app.emotion.summary import most_common_emotion, print_emotion_summary

# ì‹¤í–‰ ìƒíƒœ ë° í ì´ˆê¸°í™”
running_event = threading.Event()
running_event.set()
emotion_logs = deque(maxlen=10)
analysis_queue = Queue(maxsize=1)

# ê°ì • í•©ì„± í•¨ìˆ˜ëŠ” app.emotion.synthesizer ëª¨ë“ˆë¡œ ì´ë™ë˜ì—ˆìŠµë‹ˆë‹¤

def analyze_loop(vlm_model, processor, device, whisper_model):
    while running_event.is_set():
        if not analysis_queue.empty():
            try:
                frame = analysis_queue.get()
                image = Image.fromarray(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))

                # ì–¼êµ´ ê°ì • ë¶„ì„
                face_emotion = analyze_facial_expression(image)

                # ìŒì„± ë…¹ìŒ ë° í…ìŠ¤íŠ¸ ë³€í™˜, ëª©ì†Œë¦¬í†¤ ê°ì • ì¶”ì¶œ
                file_path = record_audio()
                text = transcribe_audio(whisper_model, file_path)
                voice_tone_emotion = analyze_voice_emotion_korean(file_path)

                # SmolVLMìœ¼ë¡œ ë°°ê²½/ì¥ë©´ ì¸ì‹
                scene_context = summarize_scene(image, processor, vlm_model, device)

                # ë¡œê·¸ ì €ì¥
                emotion_logs.append({
                    "face": face_emotion,
                    "text": text,
                    "voice_emotion": voice_tone_emotion,
                    "scene": scene_context
                })

                print(f"[í‘œì • ê°ì •] {face_emotion} / [ëª©ì†Œë¦¬í†¤ ê°ì •] {voice_tone_emotion}")
                print(f"[ì‚¬ìš©ì ë°œí™”] {text}")
                print(f"[í˜„ì¬ ì¥ë©´ ìš”ì•½] {scene_context}")

            except Exception as e:
                print(f"[â—ì˜ˆì™¸] {e}")
        time.sleep(0.2)

    # ì¢…ë£Œ í›„ ë¶„ì„ ë° Gemini ì‘ë‹µ
    if emotion_logs:
        all_faces = [e["face"] for e in emotion_logs]
        all_texts = " ".join([e["text"] for e in emotion_logs])
        all_voice_emotions = [e["voice_emotion"] for e in emotion_logs]
        last_scene = emotion_logs[-1]["scene"]

        final_face = most_common_emotion(all_faces)
        final_voice_emotion = most_common_emotion(all_voice_emotions)

        context = {
            "weather": "ë§‘ìŒ",
            "sleep": "7ì‹œê°„",
            "stress": "ì¤‘ê°„",
            "location_scene": last_scene,
            "emotion_history": [final_face, final_voice_emotion]
        }

        print_emotion_summary(emotion_logs)

        # ê°ì • í•©ì„± (ê°„ë‹¨í•œ ì²˜ë¦¬)
        emotion = final_face if final_face != "unknown" else final_voice_emotion

        # ë¹„ë™ê¸° í•¨ìˆ˜ í˜¸ì¶œì„ ìœ„í•œ ì²˜ë¦¬
        import asyncio
        try:
            response = asyncio.run(generate_response(
                emotion=emotion,
                user_text=all_texts,
                context=context,
                ai_mbti_persona=None  # ê¸°ë³¸ í˜ë¥´ì†Œë‚˜ ì‚¬ìš©
            ))
        except Exception as e:
            print(f"ì‘ë‹µ ìƒì„± ì¤‘ ì˜¤ë¥˜: {e}")
            response = "ì£„ì†¡í•©ë‹ˆë‹¤, ì‘ë‹µì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
            
        print("\nğŸ§  Gemini ì‘ë‹µ:")
        print(response)
    else:
        print("â— ë¶„ì„ëœ ê°ì • ë¡œê·¸ê°€ ì—†ìŠµë‹ˆë‹¤.")

def analyze_image(image):
    """ì´ë¯¸ì§€ ë¶„ì„ ì²˜ë¦¬ í•¨ìˆ˜"""
    face_emotion = analyze_facial_expression(image)
    
    # ì´ë¯¸ì§€ë¥¼ ë¶„ì„ íì— ì¶”ê°€ (ë°±ê·¸ë¼ìš´ë“œ ë¶„ì„ìš©)
    frame = cv2.cvtColor(np.array(image), cv2.COLOR_RGB2BGR)
    if not analysis_queue.full():
        analysis_queue.put(frame)
    
    return face_emotion
