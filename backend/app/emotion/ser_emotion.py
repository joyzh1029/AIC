# import subprocess
# import os
# import torch
# import torchaudio
# import matplotlib.pyplot as plt
# from transformers import Wav2Vec2ForSequenceClassification, Wav2Vec2FeatureExtractor

# # ëª¨ë¸ ë¶ˆëŸ¬ì˜¤ê¸°
# model_name = "jungjongho/wav2vec2-xlsr-korean-speech-emotion-recognition"
# model = Wav2Vec2ForSequenceClassification.from_pretrained(model_name)
# extractor = Wav2Vec2FeatureExtractor.from_pretrained(model_name)

# device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
# model.to(device)
# model.eval()

# def convert_webm_to_wav(input_path, output_path):
#     command = [
#         "ffmpeg", "-y", "-i", input_path, "-ar", "16000", output_path
#     ]
#     subprocess.run(command, stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)

# def analyze_voice_emotion_korean(file_path: str) -> str:
#     wav_path = "converted_temp.wav"
#     convert_webm_to_wav(file_path, wav_path)

#     # WAV ë¡œë”©
#     speech_array, sampling_rate = torchaudio.load(wav_path)
#     print("ğŸ§ [INFO] Sampling rate:", sampling_rate)
#     print("ğŸ§ [INFO] Wave shape:", speech_array.shape)
#     print("ğŸ§ [INFO] ì²« 10ê°œ ìƒ˜í”Œ:", speech_array[0][:10])

#     # waveform ì‹œê°í™”
#     plt.figure(figsize=(10, 3))
#     plt.plot(speech_array[0].numpy())
#     plt.title("ğŸ“ˆ Waveform (Debug)")
#     plt.savefig("waveform_debug.png")
#     plt.close()

#     # ëª¨ë¸ ì…ë ¥ ì¤€ë¹„
#     inputs = extractor(
#         speech_array.squeeze(), 
#         sampling_rate=sampling_rate, 
#         return_tensors="pt", 
#         padding=True
#     )
#     inputs = inputs.to(device)

#     # ì¶”ë¡ 
#     with torch.no_grad():
#         logits = model(**inputs).logits
#         predicted = torch.argmax(logits, dim=-1)

#     print("ğŸ“Š [LOGITS]", logits.cpu().numpy())
#     print("ğŸ” [PREDICTED]", predicted.item())

#     label = model.config.id2label[predicted.item()]

#     # ì²­ì†Œ
#     if os.path.exists(wav_path):
#         os.remove(wav_path)

#     return label

import subprocess
import os
import torchaudio
import matplotlib.pyplot as plt
import torch

# 1. ëª¨ë¸ ë¡œë“œ
from funasr import AutoModel

# âœ… ì •í™•í•œ ëª¨ë¸ ê²½ë¡œë¡œ ìˆ˜ì •
model_dir = "FunAudioLLM/SenseVoiceSmall"

model = AutoModel(
    model=model_dir,
    vad_model="FunAudioLLM/SenseVoiceSmall",  # ä½¿ç”¨ç›¸åŒçš„æ¨¡å‹ä½œä¸ºVAD
    vad_kwargs={
        "max_single_segment_time": 30000,
        "disable_update": True,  # ç¦ç”¨æ›´æ–°æ£€æŸ¥
        "model_type": "vad",  # æŒ‡å®šæ¨¡å‹ç±»å‹ä¸ºVAD
        "hub": "hf"  # ä½¿ç”¨HuggingFaceä½œä¸ºæ¨¡å‹æº
    },
    device="cuda:0" if torch.cuda.is_available() else "cpu",
    hub="hf",  # ä½¿ç”¨HuggingFaceä½œä¸ºæ¨¡å‹æº
)

print("ğŸ§  [INFO] sensevoice_small ëª¨ë¸ ë¡œë”© ì™„ë£Œ")

# 2. webm â†’ wav ë³€í™˜
def convert_webm_to_wav(input_path, output_path):
    command = [
        "ffmpeg", "-y", "-i", input_path, "-ar", "16000", "-ac", "1", output_path
    ]
    subprocess.run(command, stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)

# 3. ê°ì • ë¶„ì„ í•¨ìˆ˜
def analyze_voice_emotion_korean(file_path: str) -> str:
    # åªå¯¹é wav æ–‡ä»¶åšè½¬æ¢
    if file_path.endswith('.wav'):
        wav_path = file_path
    else:
        wav_path = "converted_temp.wav"
        convert_webm_to_wav(file_path, wav_path)

    # ë””ë²„ê¹…ìš© waveform ì‹œê°í™”
    waveform, sr = torchaudio.load(wav_path)
    print("ğŸ§ [INFO] Sampling rate:", sr)
    plt.figure(figsize=(10, 3))
    plt.plot(waveform[0].numpy())
    plt.title("ğŸ“ˆ Waveform (Debug)")
    plt.savefig("waveform_debug.png")
    plt.close()

    # 4. FunASR ëª¨ë¸ë¡œ ì¶”ë¡ 
    result = model.generate(input=wav_path)
    print("ğŸ” [MODEL RESULT]", result)

    # 5. ë¦¬ìŠ¤íŠ¸ ë°˜í™˜ ëŒ€ì‘
    if isinstance(result, list) and len(result) > 0:
        result = result[0]
    elif not isinstance(result, dict):
        result = {}

    # 6. ê°ì • ê²°ê³¼ ì¶”ì¶œ
    emotion = result.get("emotion", "unknown")
    # env_text = result.get("text", "unknown")  # ì‚¬ìš©í•˜ì§€ ì•Šì„ ê²½ìš° ìƒëµ ê°€ëŠ¥

    # 7. ì„ì‹œ íŒŒì¼ ì •ë¦¬
    if wav_path != file_path and os.path.exists(wav_path):
        os.remove(wav_path)

    return emotion  # ë˜ëŠ” return emotion, env_text