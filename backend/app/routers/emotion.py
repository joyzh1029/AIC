from fastapi import APIRouter, UploadFile, File, Form
from fastapi.responses import JSONResponse
from PIL import Image
import io
import subprocess
import os

# ê°ì • ë¶„ì„ ëª¨ë“ˆ import (ì˜ˆì™¸ ì²˜ë¦¬ì— ì§‘ì¤‘í•˜ë¯€ë¡œ ì‹¤ì œ ë¡œì§ì€ ê°„ë‹¨í™”)
from app.emotion.fer_emotion import analyze_facial_expression
from app.emotion.ser_emotion import analyze_voice_emotion_korean
from app.audio.stt import transcribe_audio, load_whisper_model
from app.multimodal.vlm import summarize_scene, load_smol_vlm
from app.nlp.llm import generate_response
from app.core.data_generator import generate_environment_context

router = APIRouter(prefix="/api/emotion", tags=["emotion"])

# ëª¨ë¸ ë¯¸ë¦¬ ë¡œë”©
whisper_model = load_whisper_model()
processor, vlm_model, device = load_smol_vlm()

def convert_webm_to_wav(input_path, output_path):
    command = [
        "ffmpeg", "-y", "-i", input_path, "-ar", "16000", "-ac", "1", output_path
    ]
    subprocess.run(command, stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)

@router.post("/analyze")
async def analyze_emotion(
    image: UploadFile = File(...),
    audio: UploadFile = File(...),
    text: str = Form("")
):
    try:
        print("\nğŸ“¥ ë¶„ì„ ìš”ì²­ ìˆ˜ì‹ ")
        print("- ì´ë¯¸ì§€ íŒŒì¼ëª…:", image.filename)
        print("- ì˜¤ë””ì˜¤ íŒŒì¼ëª…:", audio.filename)
        print("- í…ìŠ¤íŠ¸ ë‚´ìš©:", text)

        # ì´ë¯¸ì§€ ì²˜ë¦¬
        image_bytes = await image.read()
        print("- ì´ë¯¸ì§€ í¬ê¸°:", len(image_bytes))
        if not image_bytes:
            raise ValueError("ì´ë¯¸ì§€ íŒŒì¼ì´ ë¹„ì–´ ìˆìŒ")

        pil_image = Image.open(io.BytesIO(image_bytes))

        # ì–¼êµ´ ê°ì • ë¶„ì„
        face_emotion = analyze_facial_expression(pil_image)
        print("âœ… ì–¼êµ´ ê°ì • ë¶„ì„ ê²°ê³¼:", face_emotion)

        # ì˜¤ë””ì˜¤ ì²˜ë¦¬
        audio_path = "temp_debug.webm"
        wav_path = "temp_debug.wav"

        with open(audio_path, "wb") as f:
            f.write(await audio.read())
        print("- ì˜¤ë””ì˜¤ ì €ì¥ ì™„ë£Œ:", audio_path)

        convert_webm_to_wav(audio_path, wav_path)
        print("- ì˜¤ë””ì˜¤ ë³€í™˜ ì™„ë£Œ:", wav_path)

        voice_emotion = analyze_voice_emotion_korean(wav_path)
        print("âœ… ìŒì„± ê°ì • ë¶„ì„ ê²°ê³¼:", voice_emotion)

        transcribed_text = transcribe_audio(whisper_model, wav_path)
        print("âœ… STT ê²°ê³¼:", transcribed_text)

        # ì¥ë©´ ë¶„ì„
        scene = summarize_scene(pil_image, processor, vlm_model, device)
        print("âœ… ì¥ë©´ ìš”ì•½:", scene)

        # í™˜ê²½ ì»¨í…ìŠ¤íŠ¸ ë°ì´í„° ìƒì„± (ì‹¤ì œ ê°ì§€ëœ ì–¼êµ´ê³¼ ìŒì„± ê°ì • ì‚¬ìš©)
        context_data = generate_environment_context(face_emotion=face_emotion, voice_emotion=voice_emotion)
        context_data["location_scene"] = scene  # ì¥ë©´ ë¶„ì„ ê²°ê³¼ ì¶”ê°€

        print(f"ìƒì„±ëœ í™˜ê²½ ë°ì´í„°: {context_data}")

        # LLM ì‘ë‹µ ìƒì„±
        final_text = transcribed_text or text or ""

        # ê°ì • í•©ì„± (ê°„ë‹¨í•œ ì²˜ë¦¬)
        emotion = face_emotion if face_emotion != "unknown" else voice_emotion

        response = await generate_response(
            emotion=emotion,
            user_text=final_text,
            context=context_data,
            ai_mbti_persona=None  # ê¸°ë³¸ í˜ë¥´ì†Œë‚˜ ì‚¬ìš©
        )
        print("âœ… Gemini ì‘ë‹µ ì™„ë£Œ")

        # ì„ì‹œ íŒŒì¼ ì •ë¦¬
        if os.path.exists(audio_path):
            os.remove(audio_path)
        if os.path.exists(wav_path):
            os.remove(wav_path)

        return JSONResponse(content={
            "success": True,
            "face": face_emotion,
            "voice": voice_emotion,
            "scene": scene,
            "text": final_text,
            "response": response
        })

    except Exception as e:
        import traceback
        print("âŒ ì˜¤ë¥˜ ë°œìƒ:")
        traceback.print_exc()
        return JSONResponse(status_code=500, content={"success": False, "error": str(e)})

