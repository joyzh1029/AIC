from fastapi import APIRouter, UploadFile, File, Form
from fastapi.responses import JSONResponse
from PIL import Image
import io

from app.audio.stt import transcribe_audio, load_whisper_model
from app.emotion.fer_emotion import analyze_facial_expression
from app.emotion.ser_emotion import analyze_voice_emotion_korean
from app.multimodal.vlm import summarize_scene, load_smol_vlm
from app.nlp.llm import generate_response

router = APIRouter(prefix="/api/conversation", tags=["conversation"])

# ëª¨ë¸ ë¡œë”©
whisper_model = load_whisper_model()
processor, vlm_model, device = load_smol_vlm()

def is_search_query(text: str) -> bool:
    keywords = ["ì¶”ì²œ", "ë¹„êµ", "ì •ë³´", "ì•Œê³  ì‹¶", "ë¦¬ë·°", "ì–´ë–¤", "ê²€ìƒ‰", "ì‹ ì œí’ˆ"]
    return any(kw in text for kw in keywords)

@router.post("/respond")
async def unified_conversation_endpoint(
    image: UploadFile = File(...),
    audio: UploadFile = File(...),
    text: str = Form("")
):
    try:
        # Step 1: í…ìŠ¤íŠ¸ í™•ë³´
        audio_path = "temp_audio.webm"
        with open(audio_path, "wb") as f:
            f.write(await audio.read())
        stt_text = transcribe_audio(whisper_model, audio_path)
        user_text = stt_text or text or ""

        # Step 2: ê²€ìƒ‰ ì˜ë„ ê°ì§€ (ì²˜ë¦¬ëŠ” í”„ë¡ íŠ¸ì—ì„œ /search/queryë¡œ ë¶„ê¸°)
        if is_search_query(user_text):
            print("ğŸ” ê²€ìƒ‰ ì§ˆì˜ ê°ì§€ë¨. í”„ë¡ íŠ¸ì—”ë“œì—ì„œ /search/query ë¡œ ë¶„ê¸° ìš”ë§.")
            return JSONResponse(content={
                "success": False,
                "mode": "search_detected",
                "message": "ê²€ìƒ‰ ì§ˆì˜ë¡œ ê°ì§€ë¨. /search/query ì—”ë“œí¬ì¸íŠ¸ë¥¼ ì‚¬ìš©í•˜ì„¸ìš”.",
                "text": user_text
            })

        # Step 3: ê°ì • ë¶„ì„
        image_bytes = await image.read()
        pil_image = Image.open(io.BytesIO(image_bytes))
        face_emotion = analyze_facial_expression(pil_image)
        voice_emotion = analyze_voice_emotion_korean(audio_path)
        scene = summarize_scene(pil_image, processor, vlm_model, device)

        context = {
            "location_scene": scene,
            "emotion_history": [face_emotion, voice_emotion]
        }

        response = generate_response(
            face_emotion=face_emotion,
            voice_emotion=voice_emotion,
            scene=scene,
            user_text=user_text,
            context=context
        )

        return JSONResponse(content={
            "success": True,
            "mode": "emotion",
            "response": response,
            "face": face_emotion,
            "voice": voice_emotion,
            "scene": scene,
            "text": user_text
        })

    except Exception as e:
        import traceback
        traceback.print_exc()
        return JSONResponse(status_code=500, content={"success": False, "error": str(e)})

