from langchain_core.messages import HumanMessage
from langchain_google_genai import ChatGoogleGenerativeAI
import google.generativeai as genai
import os
import logging
from typing import Optional
from ..nlp.response_formatter import format_llm_response
from ..core.mbti_data import MBTI_PERSONAS

# âœ¨ ì „ì—­ Gemini ëª¨ë¸ ë³€ìˆ˜ ì¶”ê°€
_gemini_model = None

def configure_gemini():
    # í™˜ê²½ë³€ìˆ˜ì—ì„œ Google API í‚¤ ê°€ì ¸ì˜¤ê¸°
    api_key = os.getenv("GOOGLE_API_KEY")
    if not api_key:
        raise ValueError("GOOGLE_API_KEY í™˜ê²½ë³€ìˆ˜ê°€ ì—†ìŠµë‹ˆë‹¤.")
    genai.configure(api_key=api_key)
    
    # âœ¨ ì „ì—­ ëª¨ë¸ ì´ˆê¸°í™”
    global _gemini_model
    try:
        _gemini_model = genai.GenerativeModel('gemini-2.0-flash')
        logging.info("Gemini model initialized successfully")
    except Exception as e:
        logging.error(f"Failed to initialize Gemini model: {e}")
        raise

async def generate_response(
    emotion: str,
    user_text: str,
    context: dict,
    ai_mbti_persona: Optional[str] = None, # âœ¨ ai_mbti_persona ì¸ì ì¶”ê°€
    model_name: str = "gemini-2.0-flash"
):
    global _gemini_model
    if _gemini_model is None:
        logging.error("Gemini model is not initialized. Attempting to initialize now (this is not ideal).")
        try:
            configure_gemini()
            if _gemini_model is None: # ì¬ì´ˆê¸°í™” ì‹¤íŒ¨ ì‹œ
                raise RuntimeError("Failed to re-initialize Gemini model.")
        except Exception as e:
            logging.error(f"Critical error: Gemini model not available for response generation: {e}", exc_info=True)
            return "ì£„ì†¡í•©ë‹ˆë‹¤, AI ëª¨ë¸ì„ ì´ˆê¸°í™”í•  ìˆ˜ ì—†ì–´ ì‘ë‹µì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
    
    # âœ¨ ê³µí†µ ì§€ì‹œì‚¬í•­ ì •ì˜
    common_directives = (
        "ì¹œêµ¬ì™€ì˜ ì¼ìƒì ì¸ ëŒ€í™”ì²˜ëŸ¼ ê°„ê²°í•˜ê³  ì§ì ‘ì ì¸ ë°˜ë§ íˆ¬ë¡œ ì†Œí†µí•´.\n"
        "ë‹µë³€ì€ 2-3ê°œì˜ ì§§ì€ ë¬¸ì¥ìœ¼ë¡œ ë‚˜ëˆ„ì–´ ë³´ë‚´ë©° ê° ë¬¸ì¥ ì‚¬ì´ì— [ë¶„í• ]ì´ë¼ëŠ” êµ¬ë¶„ìë¥¼ ë„£ì–´."
    )

    # âœ¨ MBTI í˜ë¥´ì†Œë‚˜ ì ìš© (ai_mbti_personaê°€ ì œê³µë˜ë©´ ì¶”ê°€)
    if ai_mbti_persona:
        mbti_description = MBTI_PERSONAS.get(ai_mbti_persona, f"{ai_mbti_persona}")
        full_persona = f"{mbti_description}\n{common_directives}\n"
        logging.info(f"Applying AI persona: {mbti_description}")
    else:
        full_persona = f"ë„ˆëŠ” ê°ì •ì— ê³µê°í•˜ê³  ìœ„ë¡œí•˜ëŠ” AIì•¼.\n{common_directives}\n"
        logging.info("Applying default AI persona.")
    
    # ì‚¬ìš©ìì˜ ê°ì • ìƒíƒœ, ë°œí™”, ì»¨í…ìŠ¤íŠ¸ ì •ë³´ë¥¼ ê¸°ë°˜ìœ¼ë¡œ í”„ë¡¬í”„íŠ¸ êµ¬ì„±
    prompt = (
        f"{full_persona}\n"
        f"ì‚¬ìš©ìëŠ” í˜„ì¬ '{emotion}' ìƒíƒœì•¼.\n"
        f"ì‚¬ìš©ìì˜ ë°œí™” ë‚´ìš©ì€: \"{user_text}\"\n\n"
        f"ë‚ ì”¨: {context.get('weather', 'ì•Œ ìˆ˜ ì—†ìŒ')}\n"
        f"ìˆ˜ë©´ ì‹œê°„: {context.get('sleep', 'ì•Œ ìˆ˜ ì—†ìŒ')}\n"
        f"ìŠ¤íŠ¸ë ˆìŠ¤ ìˆ˜ì¤€: {context.get('stress', 'ì•Œ ìˆ˜ ì—†ìŒ')}\n"
        f"ìµœê·¼ ê°ì • íë¦„: {context.get('emotion_history', [])}\n\n"
        "ìœ„ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ìœ„ë¡œ í˜¹ì€ ê³µê°ì˜ í•œ ë§ˆë””ë¥¼ ìì—°ìŠ¤ëŸ½ê³  ì¸ê°„ì ìœ¼ë¡œ ì „ë‹¬í•´ì¤˜.\n"
        "ê°€ë³ê²Œ ì§ˆë¬¸ í•˜ë‚˜ë¡œ ë§ˆë¬´ë¦¬í•´ë„ ì¢‹ì•„."
    )
    logging.info(f"Generated prompt for Gemini: {prompt}") # âœ¨ ìƒì„±ëœ í”„ë¡¬í”„íŠ¸ ë¡œê¹… (í•µì‹¬ 1)

    try:
        # âœ¨ Gemini ëª¨ë¸ í˜¸ì¶œ ì§ì „ ë¡œê¹…
        logging.info("Gemini ëª¨ë¸ í˜¸ì¶œ ì¤‘...")
        response = await _gemini_model.generate_content_async(prompt)

        # âœ¨ Gemini ì‘ë‹µ ê°ì²´ í™•ì¸ ë¡œê¹…
        logging.info(f"Gemini raw response object type: {type(response)}") # ì‘ë‹µ ê°ì²´ íƒ€ì…
        logging.info(f"Gemini raw response parts: {response.parts if hasattr(response, 'parts') else 'N/A'}") # ì‘ë‹µ parts
        logging.info(f"Gemini raw response text: {response.text.strip()}") # âœ¨ ì‹¤ì œ í…ìŠ¤íŠ¸ ì‘ë‹µ (í•µì‹¬ 2)

        # ì‘ë‹µ í…ìŠ¤íŠ¸ë§Œ ë°˜í™˜
        return response.text.strip().replace("[ë¶„í• ]", "\n")
    except Exception as e:
        logging.error(f"LLM ì‘ë‹µ ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}", exc_info=True)
        return "ì£„ì†¡í•©ë‹ˆë‹¤, í˜„ì¬ ì‘ë‹µì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."

# âœ¨ ê¸°ì¡´ í•¨ìˆ˜ì™€ì˜ í˜¸í™˜ì„±ì„ ìœ„í•œ ë˜í¼ í•¨ìˆ˜
def generate_response_sync(
    face_emotion: str,
    voice_emotion: str,
    scene: str,
    user_text: str,
    context: dict,
    model_name="gemini-2.0-flash",
    vlm_analysis: str = None,
    ai_mbti_persona: Optional[str] = None
):
    """
    ê¸°ì¡´ ì½”ë“œì™€ì˜ í˜¸í™˜ì„±ì„ ìœ„í•œ ë™ê¸° ë˜í¼ í•¨ìˆ˜
    """
    import asyncio
    
    # ê°ì • í•©ì„± (ê¸°ì¡´ ë¡œì§ ìœ ì§€)
    emotion = face_emotion  # ê°„ë‹¨í•œ ì²˜ë¦¬, í•„ìš”ì‹œ ë” ë³µì¡í•œ ë¡œì§ ì ìš©
    
    # VLM ë¶„ì„ ê²°ê³¼ê°€ ìˆëŠ” ê²½ìš° ì»¨í…ìŠ¤íŠ¸ì— ì¶”ê°€
    if vlm_analysis:
        context["vlm_analysis"] = vlm_analysis
    
    # ë¹„ë™ê¸° í•¨ìˆ˜ í˜¸ì¶œ
    try:
        loop = asyncio.get_event_loop()
        if loop.is_running():
            # ì´ë¯¸ ì‹¤í–‰ ì¤‘ì¸ ë£¨í”„ê°€ ìˆìœ¼ë©´ ìƒˆ ìŠ¤ë ˆë“œì—ì„œ ì‹¤í–‰
            import concurrent.futures
            with concurrent.futures.ThreadPoolExecutor() as executor:
                future = executor.submit(asyncio.run, generate_response(
                    emotion=emotion,
                    user_text=user_text,
                    context=context,
                    ai_mbti_persona=ai_mbti_persona,
                    model_name=model_name
                ))
                return future.result()
        else:
            return asyncio.run(generate_response(
                emotion=emotion,
                user_text=user_text,
                context=context,
                ai_mbti_persona=ai_mbti_persona,
                model_name=model_name
            ))
    except Exception as e:
        logging.error(f"Error in generate_response_sync: {str(e)}")
        return "ì£„ì†¡í•©ë‹ˆë‹¤. ì‘ë‹µì„ ìƒì„±í•˜ëŠ” ì¤‘ì— ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."

def generate_search_summary(user_text: str, raw_results: list[str]) -> str:
    print("ğŸ” ê²€ìƒ‰ ìš”ì•½ ìƒì„±")
    combined = "\n\n".join(raw_results)
    prompt = (
        f"ë„ˆëŠ” ì •ë³´ ê²€ìƒ‰ ê²°ê³¼ë¥¼ ìš”ì•½ ë° ë²ˆì—­í•´ì£¼ëŠ” ì „ë¬¸ AIì•¼.\n\n"
        f"ì‚¬ìš©ìì˜ ì§ˆë¬¸ì€ ë‹¤ìŒê³¼ ê°™ì•„:\n"
        f"\"{user_text}\"\n\n"
        f"ê²€ìƒ‰ ê²°ê³¼ëŠ” ì•„ë˜ì™€ ê°™ì•„:\n"
        f"{combined}\n\n"
        "ì´ ê²€ìƒ‰ ê²°ê³¼ë“¤ì„ í•­ëª©ë³„ë¡œ ì •ëˆí•´ì„œ ìš”ì•½í•˜ê³ , ì´í•´í•˜ê¸° ì‰½ê²Œ í•œêµ­ì–´ë¡œ ë²ˆì—­í•´ì¤˜. "
        "ì¹´í…Œê³ ë¦¬(ì˜ˆ: ìœ ì„  í‚¤ë³´ë“œ, ë¬´ì„  í‚¤ë³´ë“œ ë“±) ë³„ë¡œ ì •ë¦¬í•˜ë©´ ë” ì¢‹ì•„."
    )

    try:
        llm = ChatGoogleGenerativeAI(
            model="gemini-1.5-flash",
            temperature=0.7,
            api_key=os.getenv("GOOGLE_API_KEY"),
        )
        response = llm.invoke(prompt)
        return format_llm_response(response)
    except Exception as e:
        logging.error(f"Error in generate_search_summary: {str(e)}")
        return "ì£„ì†¡í•©ë‹ˆë‹¤. ê²€ìƒ‰ ê²°ê³¼ë¥¼ ìš”ì•½í•˜ëŠ” ì¤‘ì— ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."

