from langchain_core.messages import HumanMessage
from langchain_google_genai import ChatGoogleGenerativeAI
import google.generativeai as genai
import os
import logging
from app.nlp.response_formatter import format_llm_response

def configure_gemini():
    # í™˜ê²½ë³€ìˆ˜ì—ì„œ Google API í‚¤ ê°€ì ¸ì˜¤ê¸°
    api_key = os.getenv("GOOGLE_API_KEY")
    if not api_key:
        raise ValueError("GOOGLE_API_KEY í™˜ê²½ë³€ìˆ˜ê°€ ì—†ìŠµë‹ˆë‹¤.")
    genai.configure(api_key=api_key)

def generate_response(
    face_emotion: str,
    voice_emotion: str,
    scene: str,
    user_text: str,
    context: dict,
    model_name="gemini-2.0-flash"
):
    if "search_raw_list" in context:
        print("ğŸ” ê²€ìƒ‰ ì‘ë‹µ ìƒì„±")
        raw_list = context["search_raw_list"]
        combined = "\n\n".join(raw_list)
        prompt = (
            f"ë„ˆëŠ” ì •ë³´ ê²€ìƒ‰ ê²°ê³¼ë¥¼ ìš”ì•½ ë° ë²ˆì—­í•´ì£¼ëŠ” ì „ë¬¸ AIì•¼.\n\n"
            f"ì‚¬ìš©ìì˜ ì§ˆë¬¸ì€ ë‹¤ìŒê³¼ ê°™ì•„:\n"
            f"\"{user_text}\"\n\n"
            f"ê²€ìƒ‰ ê²°ê³¼ëŠ” ì•„ë˜ì™€ ê°™ì•„:\n"
            f"{combined}\n\n"
            "ì´ ê²€ìƒ‰ ê²°ê³¼ë“¤ì„ í•­ëª©ë³„ë¡œ ì •ëˆí•´ì„œ ìš”ì•½í•˜ê³ , ì´í•´í•˜ê¸° ì‰½ê²Œ í•œêµ­ì–´ë¡œ ë²ˆì—­í•´ì¤˜. "
            "ì¹´í…Œê³ ë¦¬(ì˜ˆ: ìœ ì„  í‚¤ë³´ë“œ, ë¬´ì„  í‚¤ë³´ë“œ ë“±) ë³„ë¡œ ì •ë¦¬í•˜ë©´ ë” ì¢‹ì•„."
        )
    else:
        print("ğŸ’¬ ê°ì • ì‘ë‹µ ìƒì„±")
        prompt = (
            f"ë„ˆëŠ” ê°ì •ì— ê³µê°í•˜ê³  ìœ„ë¡œí•˜ëŠ” AIì•¼.\n"
            f"ì‚¬ìš©ìëŠ” í˜„ì¬ ì´ëŸ° ìƒíƒœì•¼:\n"
            f"- í‘œì • ê°ì •: '{face_emotion}'\n"
            f"- ëª©ì†Œë¦¬ ê°ì •: '{voice_emotion}'\n"
            f"- ì£¼ë³€ í™˜ê²½ì€ ë‹¤ìŒê³¼ ê°™ì•„: '{scene}'\n"
            f"ë°œí™” ë‚´ìš©: \"{user_text}\"\n\n"
            f"ë‚ ì”¨: {context.get('weather', 'ì•Œ ìˆ˜ ì—†ìŒ')}\n"
            f"ìˆ˜ë©´ ì‹œê°„: {context.get('sleep', 'ì•Œ ìˆ˜ ì—†ìŒ')}\n"
            f"ìŠ¤íŠ¸ë ˆìŠ¤ ìˆ˜ì¤€: {context.get('stress', 'ì•Œ ìˆ˜ ì—†ìŒ')}\n"
            f"ìµœê·¼ ê°ì • íë¦„: {context.get('emotion_history', [])}\n\n"
            "ìœ„ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì¸ê°„ì ì¸ ìœ„ë¡œ ë˜ëŠ” ê³µê°ì˜ ë©”ì‹œì§€ë¥¼ ë§Œë“¤ì–´ì¤˜.\n"
            "ë§ˆë¬´ë¦¬ë¡œ ê°€ë²¼ìš´ ì§ˆë¬¸ í•˜ë‚˜ë„ ê³ë“¤ì´ë©´ ì¢‹ì•„."
        )

    try:
        llm = ChatGoogleGenerativeAI(
            model=model_name,
            temperature=0.7,
            api_key=os.getenv("GEMINI_API_KEY"),
        )
        response = llm.invoke(prompt)
        
        # ë””ë²„ê·¸ ë¡œê¹… ì¶”ê°€
        logging.debug(f"Raw response type: {type(response)}")
        logging.debug(f"Raw response: {response}")
        
        # response ê°ì²´ ìì²´ë¥¼ í¬ë§·íŒ…
        formatted_response = format_llm_response(response)
        logging.debug(f"Formatted response: {formatted_response}")
        
        return formatted_response
    except Exception as e:
        logging.error(f"Error in generate_response: {str(e)}")
        return "ì£„ì†¡í•©ë‹ˆë‹¤. ì‘ë‹µì„ ìƒì„±í•˜ëŠ” ì¤‘ì— ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."


def generate_search_summary(user_text: str, raw_results: list[str]) -> str:
    print("ğŸ” ê²€ìƒ‰ ìš”ì•½ ìƒì„±")
    combined = "\n\n".join(raw_results)
    prompt = (
        f"ë„ˆëŠ” ì •ë³´ ê²€ìƒ‰ ê²°ê³¼ë¥¼ ìš”ì•½ ë° ë²ˆì—­í•´ì£¼ëŠ” ì „ë¬¸ AIì•¼.\n\n"
        f"ì‚¬ìš©ìì˜ ì§ˆë¬¸ì€ ë‹¤ìŒê³¼ ê°™ì•„:\n"
        f"\"{user_text}\"\n\n"
        f"ê²€ìƒ‰ ê²°ê³¼ëŠ” ì•„ë˜ì™€ ê°™ì•„:\n"
        f"{combined}\n\n"
        "ì´ ê²€ìƒ‰ ê²°ê³¼ë“¤ì„ í•­ëª©ë³„ë¡œ ì •ëˆí•´ì„œ ìš”ì•½í•˜ê³ , ì´í•´í•˜ê¸° ì‰½ê²Œ í•œêµ­ì–´ë¡œ ë²ˆì—­í•´ì¤˜. "
        "ì¹´í…Œê³ ë¦¬(ì˜ˆ: ìœ ì„  í‚¤ë³´ë“œ, ë¬´ì„  í‚¤ë³´ë“œ ë“±) ë³„ë¡œ ì •ë¦¬í•˜ë©´ ë” ì¢‹ì•„."
    )

    try:
        llm = ChatGoogleGenerativeAI(
            model="gemini-1.5-flash",
            temperature=0.7,
            api_key=os.getenv("GEMINI_API_KEY"),
        )
        response = llm.invoke(prompt)
        return format_llm_response(response)
    except Exception as e:
        logging.error(f"Error in generate_search_summary: {str(e)}")
        return "ì£„ì†¡í•©ë‹ˆë‹¤. ê²€ìƒ‰ ê²°ê³¼ë¥¼ ìš”ì•½í•˜ëŠ” ì¤‘ì— ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."

def test_llm_simple():
    llm = ChatGoogleGenerativeAI(
        model="gemini-2.0-flash",
        temperature=0.7,
        api_key=os.getenv("GEMINI_API_KEY"),
    )
    try:
        prompt = "ì•ˆë…•! ìê¸°ì†Œê°œ í•´ì¤˜."
        response = llm.invoke(prompt)
        print("Simple prompt response:", response)
    except Exception as e:
        import traceback
        print("Simple prompt error:", e)
        traceback.print_exc()
