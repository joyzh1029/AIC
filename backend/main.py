import cv2
import threading
import time
import base64
import io
import json
import numpy as np
import os
import sys
from collections import deque
from queue import Queue

# í˜„ì¬ ë””ë ‰í† ë¦¬ë¥¼ Python ê²½ë¡œì— ì¶”ê°€ (ìƒëŒ€ ê²½ë¡œ ì„í¬íŠ¸ë¥¼ ìœ„í•¨)
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from fastapi import FastAPI, Request, File, UploadFile, Form
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import JSONResponse

# ìƒëŒ€ ê²½ë¡œë¡œ ì„í¬íŠ¸ ë³€ê²½
from app.vision.webcam import capture_webcam_image
from app.multimodal.vlm import load_smol_vlm, analyze_face_emotion
from app.audio.stt import load_whisper_model, transcribe_stream
from app.vision.fer_emotion import analyze_facial_expression
from app.emotion.emotion import synthesize_emotion
from app.nlp.llm import configure_gemini, generate_response
from app.emotion.summary import most_common_emotion, print_emotion_summary
from PIL import Image

# FastAPI ì•± ì´ˆê¸°í™”
app = FastAPI()

# CORS ì„¤ì •
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# ì‹¤í–‰ ìƒíƒœ ë° í ì´ˆê¸°í™”
running_event = threading.Event()
running_event.set()
emotion_logs = deque(maxlen=10)
analysis_queue = Queue(maxsize=1)

def webcam_thread():
    cap = cv2.VideoCapture(0)
    if not cap.isOpened():
        print("ì›¹ìº ì„ ì—´ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return

    print("ì›¹ìº  ì°½ ì—´ë¦¼. 'q'ë¡œ ë¶„ì„ ìš”ì²­, 't'ë¡œ ì¢…ë£Œ.")

    while running_event.is_set():
        ret, frame = cap.read()
        if not ret:
            continue
        cv2.imshow("Webcam - Press 'q' to analyze once, 't' to terminate", frame)

        key = cv2.waitKey(1) & 0xFF
        if key == ord('q'):
            if not analysis_queue.full():
                analysis_queue.put(frame.copy())
                print("ğŸ¬ ë¶„ì„ ìš”ì²­ì´ íì— ë“±ë¡ë¨.")
            else:
                print("â³ ì´ì „ ë¶„ì„ì´ ì•„ì§ ëë‚˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        elif key == ord('t'):
            running_event.clear()
            break

    cap.release()
    cv2.destroyAllWindows()

def synthesize_emotion_3way(face, text_emotion, voice_tone_emotion):
    if face != "neutral":
        return face
    elif voice_tone_emotion != "neutral":
        return voice_tone_emotion
    else:
        return text_emotion

def analyze_loop(vlm_model, processor, device, whisper_model):
    while running_event.is_set():
        if not analysis_queue.empty():
            try:
                frame = analysis_queue.get()
                image = Image.fromarray(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))

                # ì–¼êµ´ ê°ì • ë¶„ì„
                face_emotion = analyze_facial_expression(image)

                # ìŒì„±ì—ì„œ í…ìŠ¤íŠ¸, í…ìŠ¤íŠ¸ ê°ì •, ëª©ì†Œë¦¬ í†¤ ê°ì • ì¶”ì¶œ
                text, text_emotion, voice_tone_emotion = transcribe_stream(whisper_model)

                # SmolVLMìœ¼ë¡œ ë°°ê²½/ì¥ë©´ ì¸ì‹
                scene_context = analyze_face_emotion(image, processor, vlm_model, device)

                # ë¡œê·¸ ì €ì¥
                emotion_logs.append({
                    "face": face_emotion,
                    "text": text,
                    "text_emotion": text_emotion,
                    "voice_emotion": voice_tone_emotion,
                    "scene": scene_context
                })

                print(f"[í‘œì • ê°ì •] {face_emotion} / [í…ìŠ¤íŠ¸ ê°ì •] {text_emotion} / [ëª©ì†Œë¦¬í†¤ ê°ì •] {voice_tone_emotion}")
                print(f"[ì‚¬ìš©ì ë°œí™”] {text}")
                print(f"[í˜„ì¬ ì¥ë©´ ìš”ì•½] {scene_context}")

            except Exception as e:
                print(f"[â—ì˜ˆì™¸] {e}")
        time.sleep(0.2)

    # ì¢…ë£Œ í›„ ë¶„ì„ ë° Gemini ì‘ë‹µ
    if emotion_logs:
        all_faces = [e["face"] for e in emotion_logs]
        all_texts = " ".join([e["text"] for e in emotion_logs])
        all_text_emotions = [e["text_emotion"] for e in emotion_logs]
        all_voice_emotions = [e["voice_emotion"] for e in emotion_logs]
        last_scene = emotion_logs[-1]["scene"]

        final_face = most_common_emotion(all_faces)
        final_text_emotion = most_common_emotion(all_text_emotions)
        final_voice_emotion = most_common_emotion(all_voice_emotions)

        final_emotion = synthesize_emotion_3way(final_face, final_text_emotion, final_voice_emotion)

        context = {
            "weather": "ë§‘ìŒ",
            "sleep": "7ì‹œê°„",
            "stress": "ì¤‘ê°„",
            "location_scene": last_scene,
            "emotion_history": [final_face, final_text_emotion, final_voice_emotion]
        }

        print_emotion_summary(emotion_logs)

        response = generate_response(final_emotion, all_texts, context)
        print("\nğŸ§  Gemini ì‘ë‹µ:")
        print(response)
    else:
        print("â— ë¶„ì„ëœ ê°ì • ë¡œê·¸ê°€ ì—†ìŠµë‹ˆë‹¤.")

# ì¹´ë©”ë¼ ìº¡ì²˜ API ì—”ë“œí¬ì¸íŠ¸
@app.post("/api/camera/capture")
async def capture_image(request: Request):
    try:
        # ìš”ì²­ ë³¸ë¬¸ì—ì„œ ì´ë¯¸ì§€ ë°ì´í„° ê°€ì ¸ì˜¤ê¸°
        data = await request.json()
        image_data = data.get("image", "")
        
        # base64 ë””ì½”ë”©
        if image_data.startswith("data:image"):
            # data:image/jpeg;base64, ê°™ì€ ì ‘ë‘ì‚¬ ì œê±°
            image_data = image_data.split(",")[1]
        
        image_bytes = base64.b64decode(image_data)
        image = Image.open(io.BytesIO(image_bytes))
        
        # ì´ë¯¸ì§€ ë¶„ì„ ì²˜ë¦¬
        face_emotion = analyze_facial_expression(image)
        
        # ì´ë¯¸ì§€ë¥¼ ë¶„ì„ íì— ì¶”ê°€ (ë°±ê·¸ë¼ìš´ë“œ ë¶„ì„ìš©)
        frame = cv2.cvtColor(np.array(image), cv2.COLOR_RGB2BGR)
        if not analysis_queue.full():
            analysis_queue.put(frame)
        
        # ì‘ë‹µ ë°˜í™˜
        return JSONResponse(content={
            "success": True,
            "emotion": face_emotion,
            "message": "ì´ë¯¸ì§€ê°€ ì„±ê³µì ìœ¼ë¡œ ë¶„ì„ë˜ì—ˆìŠµë‹ˆë‹¤."
        })
    except Exception as e:
        return JSONResponse(
            status_code=500,
            content={"success": False, "message": f"ì´ë¯¸ì§€ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}"})

# FastAPI ì•± ì‹¤í–‰ (uvicornì—ì„œ ì‹¤í–‰í•  ë•Œ ì‚¬ìš©)
@app.on_event("startup")
def startup_event():
    # ëª¨ë¸ ë¡œë”©
    global processor, vlm_model, device, whisper_model
    print("ğŸ”§ ëª¨ë¸ ë¡œë”© ì¤‘...")
    processor, vlm_model, device = load_smol_vlm()
    whisper_model = load_whisper_model()
    configure_gemini()
    
    # ë¶„ì„ ìŠ¤ë ˆë“œ ì‹œì‘
    global analyzer
    analyzer = threading.Thread(target=analyze_loop, args=(vlm_model, processor, device, whisper_model))
    analyzer.daemon = True
    analyzer.start()

@app.on_event("shutdown")
def shutdown_event():
    # ìŠ¤ë ˆë“œ ì¢…ë£Œ ì´ë²¤íŠ¸ ì„¤ì •
    running_event.clear()
    print("ğŸ‘‹ ì¢…ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.")

if __name__ == "__main__":
    import uvicorn
    import numpy as np
    
    print("ğŸ”§ ëª¨ë¸ ë¡œë”© ì¤‘...")
    processor, vlm_model, device = load_smol_vlm()
    whisper_model = load_whisper_model()
    configure_gemini()

    # ì›¹ìº  ìŠ¤ë ˆë“œëŠ” ë…ë¦½ ì‹¤í–‰ ëª¨ë“œì—ì„œë§Œ ì‚¬ìš©
    webcam = threading.Thread(target=webcam_thread)
    analyzer = threading.Thread(target=analyze_loop, args=(vlm_model, processor, device, whisper_model))

    webcam.start()
    analyzer.start()

    # FastAPI ì•± ì‹¤í–‰
    uvicorn.run(app, host="0.0.0.0", port=8181)
    
    # ìŠ¤ë ˆë“œ ì¢…ë£Œ ëŒ€ê¸°
    webcam.join()
    analyzer.join()

    print("ğŸ‘‹ ì¢…ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.")
